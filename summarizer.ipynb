{
 "metadata": {
  "name": "",
  "signature": "sha256:878bdafa8e6dc70f9050c17349f19eb011ce39e1c5d248bc6d2d86d9516cef4c"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# -*- coding: utf8 -*-\n",
      "import os\n",
      "\n",
      "from __future__ import absolute_import\n",
      "from __future__ import division, print_function\n",
      "\n",
      "from sumy.parsers.plaintext import PlaintextParser\n",
      "from sumy.nlp.tokenizers import Tokenizer\n",
      "from sumy.summarizers.lsa import LsaSummarizer as Summarizer\n",
      "from sumy.nlp.stemmers import Stemmer\n",
      "from sumy.utils import get_stop_words\n",
      "\n",
      "#Add the following two lines if you have the following error (NLTK not installed):\n",
      "#Resource 'tokenizers/punkt/english.pickle' not found.  Please\n",
      "#use the NLTK Downloader to obtain the resource:\n",
      "#import nltk\n",
      "#nltk.download() #Opens a windows GUI installer for NLTK\n",
      "\n",
      "\n",
      "LANGUAGE = \"english\"\n",
      "SENTENCES_COUNT = 1\n",
      "\n",
      "readPath = \"C:/git/SemanticTextDB/example_code/all_US_Law_Codes/\"\n",
      "count = 0\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    for filename in os.listdir(readPath):\n",
      "        count = count + 1\n",
      "        parser = PlaintextParser.from_file(readPath + filename, Tokenizer(LANGUAGE))\n",
      "        stemmer = Stemmer(LANGUAGE)\n",
      "\n",
      "        summarizer = Summarizer(stemmer)\n",
      "        summarizer.stop_words = get_stop_words(LANGUAGE)\n",
      "           \n",
      "        summary = \". \".join([str(sentence) for sentence in summarizer(parser.document, SENTENCES_COUNT)])\n",
      "        print(summary)\n",
      "        print(\"\\n\")\n",
      "            \n",
      "        if count > 3:\n",
      "            break"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2813, provided that: \u201cExcept as expressly provided otherwise, any reference to \u2018this Act\u2019 contained in either division A [Department of Defense Appropriations Act, 2006, see Tables for classification] or division B [Emergency Supplemental Appropriations Act to Address Hurricanes in the Gulf of Mexico and Pandemic Influenza, 2006, see Tables for classification] shall be treated as referring only to the provisions of that division.\u201d\n",
        "\n",
        "\n",
        "The resolving clause of all joint resolutions shall be in the following form: \u201cResolved by the Senate and House of Representatives of the United States of America in Congress assembled.\u201d (July 30, 1947, ch.\n",
        "\n",
        "\n",
        "No enacting or resolving words shall be used in any section of an Act or resolution of Congress except in the first.\n",
        "\n",
        "\n",
        "Each section shall be numbered, and shall contain, as nearly as may be, a single proposition of enactment.\n",
        "\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import codecs\n",
      "redditReadPath = \"C:/git/SemanticTextDB/example_code/twitter.csv\"\n",
      "outPath= \"C:/git/SemanticTextDB/example_code/twitter_utf8.csv\"\n",
      "with codecs.open(redditReadPath, 'rU', 'UTF-16') as infile:\n",
      "    with open(outPath + '.utf8', 'wb') as outfile:\n",
      "        for line in infile:\n",
      "            outfile.write(line.encode('utf8'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "UnicodeError",
       "evalue": "UTF-16 stream does not start with BOM",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mUnicodeError\u001b[0m                              Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-3-2b50fdff583f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mredditReadPath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rU'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'UTF-16'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0minfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutPath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.utf8'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0moutfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m             \u001b[0moutfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Users\\C\\Anaconda\\lib\\codecs.pyc\u001b[0m in \u001b[0;36mnext\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    682\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    683\u001b[0m         \u001b[1;34m\"\"\" Return the next decoded line from the input stream.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 684\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    686\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Users\\C\\Anaconda\\lib\\codecs.pyc\u001b[0m in \u001b[0;36mnext\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    613\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m         \u001b[1;34m\"\"\" Return the next decoded line from the input stream.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 615\u001b[1;33m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    616\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Users\\C\\Anaconda\\lib\\codecs.pyc\u001b[0m in \u001b[0;36mreadline\u001b[1;34m(self, size, keepends)\u001b[0m\n\u001b[0;32m    528\u001b[0m         \u001b[1;31m# If size is given, we call read() only once\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    529\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 530\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreadsize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfirstline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    531\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m                 \u001b[1;31m# If we're at a \"\\r\" read one extra character (which might\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Users\\C\\Anaconda\\lib\\codecs.pyc\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, size, chars, firstline)\u001b[0m\n\u001b[0;32m    475\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbytebuffer\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnewdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 477\u001b[1;33m                 \u001b[0mnewchars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecodedbytes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    478\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mUnicodeDecodeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mfirstline\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Users\\C\\Anaconda\\lib\\encodings\\utf_16.pyc\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, errors)\u001b[0m\n\u001b[0;32m    110\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutf_16_be_decode\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[1;33m>=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mUnicodeError\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"UTF-16 stream does not start with BOM\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mUnicodeError\u001b[0m: UTF-16 stream does not start with BOM"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}