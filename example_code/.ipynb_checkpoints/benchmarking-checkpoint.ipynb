{
 "metadata": {
  "name": "",
  "signature": "sha256:6b688490cd3332ee40e5a3040053d2e7a0b65310588b7e891b8676a993102407"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Import system modules. Import psycopg2 (requires install). This is the python interface driver for the postgresql database."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys, os, time, csv\n",
      "import psycopg2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Add the directory of our SemanticTextDB library to your absolute path to support import dependencies in our library files."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mypath = os.path.dirname(os.path.abspath(\"__file__\"))\n",
      "repositoryLocation = \"C:/git/SemanticTextDB\"\n",
      "sys.path.insert(0, os.path.normpath(os.path.join(mypath, 'C:/git/SemanticTextDB')))\n",
      "\n",
      "import SemanticTextDB as stdb"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Create a connection to the database withe the psycopg2.connect() method. Create a cursor to execute operations. Lastly create the SemanticTextDB object. This object is the primary object you will use to interact with, query, perform statistical modelling queries, and interact with the features available with our library."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "conn = psycopg2.connect(database=\"benchmark\", user=\"Curtis Northcutt\", host=\"18.251.7.99\", password=\"coldnips\")\n",
      "cur = conn.cursor()\n",
      "\n",
      "# Now create a new SemanticTextDB object based on the underlying DB's state:\n",
      "my_stdb = stdb.SemanticTextDB(conn, cur)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Document tables in this database:  ['twitter', 'laws']\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##How to create our document tables in a postgres database"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Returns a list of all tables in underyling DB:\n",
      "my_stdb.allTables()\n",
      "\n",
      "#Returns a list of only document tables in the DB:\n",
      "my_stdb.allDocTables()\n",
      "\n",
      "# Delete the document table (iff you want to replace table with same name):\n",
      "if 'laws' in my_stdb.document_tables.keys(): #check that the table exists before deleting it\n",
      "    my_stdb.dropDocTable(\"laws\")\n",
      "    \n",
      "# Creates a document table (and associated machine-generated tables):\n",
      "my_stdb.createDocTable(\"laws\", ['lawTitleNumber text', 'lawSectionNumber text', 'lawName text'],\n",
      "                   summary = None, topics = None, entities = None, \n",
      "                   sentiment = False, count_words = False, length_count = False, \n",
      "                   vs_representations = 0, max_word_length = 200,\n",
      "                   update_increment = 1, new_transaction = False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "deleted document table: laws\n",
        "created document table: laws"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Loading in all U.S. Code Laws (over 56,000 laws) fully formatted and substantial length"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "readPath = \"C:/git/SemanticTextDB/example_code/all_US_Law_Codes/\"\n",
      "author = \"United States Government\"\n",
      "\n",
      "NUM_INTERTIMES = 1\n",
      "NUM_TRIALS = 1\n",
      "NUM_LAWS = 10001\n",
      "\n",
      "law_trials = []\n",
      "for i in range(NUM_TRIALS):\n",
      "    intertimes = []\n",
      "    count = 0\n",
      "    totaltime = 0\n",
      "    for filename in os.listdir(readPath):\n",
      "        count = count + 1\n",
      "        f = open(readPath + filename,'r')\n",
      "        fileAsString = f.read()\n",
      "        result = re.search(r'[0-9].*?\\n', fileAsString)\n",
      "        if result == None:\n",
      "            continue\n",
      "            #print fileAsString\n",
      "        else:\n",
      "            result = re.sub(r'\\n', '', result.group())\n",
      "            lawTitleNum = filename.split('_')[0]\n",
      "            lawSectionNum = (re.search(r'.*?\\.', result)).group()[:-1]\n",
      "            lawName = (re.search(r'\\..*', result)).group()[2:]\n",
      "\n",
      "            if lawName == \"\" or re.search(r'\\..*', result).group()[1] != \" \": #discard - wrong format\n",
      "                count = count - 1\n",
      "                continue\n",
      "            else:\n",
      "                start_time = time.time()\n",
      "                my_stdb.insertDoc(fileAsString, \"laws\", [lawTitleNum, lawSectionNum, lawName])\n",
      "                totaltime = totaltime + (time.time() - start_time)\n",
      "                if count % (NUM_LAWS / NUM_INTERTIMES) == 0:\n",
      "                    intertimes.append(totaltime)\n",
      "        if count >= NUM_LAWS:\n",
      "            break\n",
      "                \n",
      "    law_trials.append(intertimes)\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import csv\n",
      "\n",
      "with open(\"C:\\\\git\\\\SemanticTextDB\\\\example_code\\\\benchmark_results\\\\laws_10k_insert_only.csv\", \"wb\") as f:\n",
      "    writer = csv.writer(f)\n",
      "    writer.writerows(law_trials)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 56
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Loading up to 2.5 million twitter posts into our database"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def exception_proof_csv_reader(csv_reader):\n",
      "    '''Wraps a try-except clause around the csv reader to\n",
      "    prevent throwing of exceptions which will stop reading.'''\n",
      "    while True: \n",
      "      try: \n",
      "          yield next(csv_reader) \n",
      "      except csv.Error: \n",
      "          pass\n",
      "      continue "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Delete the document table (iff you want to replace table with same name):\n",
      "if 'twitter' in my_stdb.document_tables.keys(): #check that the table exists before deleting it\n",
      "    my_stdb.dropDocTable(\"twitter\")\n",
      "    \n",
      "# Creates a document table (and associated machine-generated tables):\n",
      "my_stdb.createDocTable(\"twitter\", ['twitterId text', 'location text', 'username text'],\n",
      "                   summary = None, topics = None, entities = None, \n",
      "                   sentiment = False, count_words = False, length_count = False, \n",
      "                   vs_representations = 0, max_word_length = 200,\n",
      "                   update_increment = 1, new_transaction = False)\n",
      "\n",
      "redditReadPath = \"C:/git/SemanticTextDB/example_code/twitter.csv\"\n",
      "\n",
      "NUM_INTERTIMES = 1\n",
      "NUM_TRIALS = 1\n",
      "NUM_TWEETS = 50001\n",
      "\n",
      "twitter_trials = []\n",
      "for i in range(NUM_TRIALS):\n",
      "    intertimes = []\n",
      "    count = 0\n",
      "    totaltime = 0\n",
      "    with open(redditReadPath, 'rU') as csvfile:\n",
      "        reader = exception_proof_csv_reader(csv.reader(csvfile, delimiter=','))\n",
      "        start_time = time.time()\n",
      "        for row in reader:\n",
      "            count = count + 1\n",
      "            try:           \n",
      "                twitterID = row.pop(0)\n",
      "                location = row.pop(0)\n",
      "                username = row.pop(-1)\n",
      "                tweet = ', '.join(row)\n",
      "            except:\n",
      "                #print \"Error occurred at item\", count, \"skipping insertion.\"\n",
      "                count = count - 1\n",
      "                continue\n",
      "            \n",
      "            start_time = time.time()\n",
      "            try:\n",
      "                my_stdb.insertDoc(tweet, \"twitter\", [twitterID, location, username])\n",
      "            except:\n",
      "                count = count - 1\n",
      "                continue\n",
      "            totaltime = totaltime + (time.time() - start_time)\n",
      "            \n",
      "            if count % (NUM_TWEETS / NUM_INTERTIMES) == 0:\n",
      "                    intertimes.append(totaltime)\n",
      "            if count >= NUM_TWEETS:\n",
      "                break\n",
      "                \n",
      "    twitter_trials.append(intertimes)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "deleted document table: twitter\n",
        "created document table: twitter"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import csv\n",
      "\n",
      "with open(\"C:\\\\git\\\\SemanticTextDB\\\\example_code\\\\benchmark_results\\\\twitter_50k_insert_only.csv\", \"wb\") as f:\n",
      "    writer = csv.writer(f)\n",
      "    writer.writerows(twitter_trials)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 55
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Benchmarking fetches"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import NLPfunctions as nlpf\n",
      "\n",
      "NUM_INTERTIMES = 10\n",
      "NUM_TRIALS = 1\n",
      "NUM_ITEMS = 11\n",
      "\n",
      "law_trials = []\n",
      "for trial in range(NUM_TRIALS):\n",
      "    intertimes = []\n",
      "    for level in range(NUM_ITEMS / NUM_INTERTIMES, NUM_ITEMS, NUM_ITEMS / NUM_INTERTIMES):\n",
      "        print level\n",
      "        start_time = time.time()\n",
      "        cur.execute(\"SELECT * FROM laws_text LIMIT \" + str(level))\n",
      "        results = cur.fetchall()\n",
      "        saResults = []\n",
      "        for item in result:\n",
      "            print item\n",
      "            #saResults.append(nlpf.sentimentAnalysis(item))\n",
      "        #except:\n",
      "            ##intertimes.append(-1) #Error occurred\n",
      "            #continue\n",
      "\n",
      "        intertimes.append(time.time() - start_time)\n",
      "    law_trials.append(intertimes)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1\n",
        "(1, '01', '1', 'Words denoting number, gender, and so forth', datetime.datetime(2014, 12, 9, 20, 9, 32, 509000))\n",
        "(2, '01', '102', 'Resolving clause', datetime.datetime(2014, 12, 9, 20, 9, 32, 540000))\n",
        "2\n",
        "(1, '01', '1', 'Words denoting number, gender, and so forth', datetime.datetime(2014, 12, 9, 20, 9, 32, 509000))\n",
        "(2, '01', '102', 'Resolving clause', datetime.datetime(2014, 12, 9, 20, 9, 32, 540000))\n",
        "3\n",
        "(1, '01', '1', 'Words denoting number, gender, and so forth', datetime.datetime(2014, 12, 9, 20, 9, 32, 509000))\n",
        "(2, '01', '102', 'Resolving clause', datetime.datetime(2014, 12, 9, 20, 9, 32, 540000))\n",
        "4\n",
        "(1, '01', '1', 'Words denoting number, gender, and so forth', datetime.datetime(2014, 12, 9, 20, 9, 32, 509000))\n",
        "(2, '01', '102', 'Resolving clause', datetime.datetime(2014, 12, 9, 20, 9, 32, 540000))\n",
        "5\n",
        "(1, '01', '1', 'Words denoting number, gender, and so forth', datetime.datetime(2014, 12, 9, 20, 9, 32, 509000))"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(2, '01', '102', 'Resolving clause', datetime.datetime(2014, 12, 9, 20, 9, 32, 540000))\n",
        "6\n",
        "(1, '01', '1', 'Words denoting number, gender, and so forth', datetime.datetime(2014, 12, 9, 20, 9, 32, 509000))\n",
        "(2, '01', '102', 'Resolving clause', datetime.datetime(2014, 12, 9, 20, 9, 32, 540000))\n",
        "7\n",
        "(1, '01', '1', 'Words denoting number, gender, and so forth', datetime.datetime(2014, 12, 9, 20, 9, 32, 509000))"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(2, '01', '102', 'Resolving clause', datetime.datetime(2014, 12, 9, 20, 9, 32, 540000))\n",
        "8\n",
        "(1, '01', '1', 'Words denoting number, gender, and so forth', datetime.datetime(2014, 12, 9, 20, 9, 32, 509000))\n",
        "(2, '01', '102', 'Resolving clause', datetime.datetime(2014, 12, 9, 20, 9, 32, 540000))\n",
        "9\n",
        "(1, '01', '1', 'Words denoting number, gender, and so forth', datetime.datetime(2014, 12, 9, 20, 9, 32, 509000))"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(2, '01', '102', 'Resolving clause', datetime.datetime(2014, 12, 9, 20, 9, 32, 540000))\n",
        "10\n",
        "(1, '01', '1', 'Words denoting number, gender, and so forth', datetime.datetime(2014, 12, 9, 20, 9, 32, 509000))\n",
        "(2, '01', '102', 'Resolving clause', datetime.datetime(2014, 12, 9, 20, 9, 32, 540000))\n"
       ]
      }
     ],
     "prompt_number": 57
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cur.execute(\"SELECT * FROM laws limit 3\")\n",
      "cur.fetchall()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 56,
       "text": [
        "[(1,\n",
        "  '01',\n",
        "  '1',\n",
        "  'Words denoting number, gender, and so forth',\n",
        "  datetime.datetime(2014, 12, 9, 20, 9, 32, 509000)),\n",
        " (2,\n",
        "  '01',\n",
        "  '102',\n",
        "  'Resolving clause',\n",
        "  datetime.datetime(2014, 12, 9, 20, 9, 32, 540000)),\n",
        " (3,\n",
        "  '01',\n",
        "  '103',\n",
        "  'Enacting or resolving words after first section',\n",
        "  datetime.datetime(2014, 12, 9, 20, 9, 32, 556000))]"
       ]
      }
     ],
     "prompt_number": 56
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Now that there is data in the database, here is an example query using pyscopg2"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Single items can be found by their unique id (deterministically the order each tuple was inserted)\n",
      "cur.execute(\"select content from twitter_text where id = 65 or id = 66;\")\n",
      "result = cur.fetchall()\n",
      "result"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "[('http://www.aurumescorts.co.uk/blog/archives/1280 London top escorts know how to treat sexual disorders',),\n",
        " ('@gareth_davies84 watched south park yet? Its pretty sick!',)]"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "statement = \"SELECT COUNT(*) FROM twitter_text WHERE content LIKE '%wedding%'\"\n",
      "posCount = my_stdb.semanticSelect('twitter_text', statement, 'positive_only', 0.8)\n",
      "negCount = my_stdb.semanticSelect('twitter_text', statement, 'negative_only', -0.8)\n",
      "if negCount != 0:\n",
      "    ratio = posCount / (1.0 * negCount)\n",
      "    print \"Ratio of support to non-support of Kate Middleton:\", ratio\n",
      "else:\n",
      "    print \"Supporter count:\", posCount\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Supporter count: 0\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "statement = \"select content from twitter_text where id < 500;\"\n",
      "my_stdb.semanticSelect('twitter_text', statement, 'positive_only', 0.8)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "[('#Jobs #Sales Deputy Manager: London-City of London,  This is a great opportunity to become Deputy Manager of ... http://bit.ly/lTgD57 #UK',),\n",
        " ('http://escort-find.com/ will help you to find the best escort girl in London to meet all your demands. All elite escorts of London are here!',),\n",
        " ('2 months of Strictly no smoking. Feels very good #nosmoking',),\n",
        " ('No Catches and just great deals - http://www.groupon.co.uk/deals/london/absolutely/344934/.dTF4Os',),\n",
        " ('Damn! 60% of chance of rain in London on Sat. What am I supposed to wear? lol #dramaqueen',)]"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "statement = \"select count(*) from twitter_text where id < 150;\"\n",
      "my_stdb.semanticSelect('twitter_text', statement, 'positive_only', 0.8)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "5"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "statement = \"select * from laws where id < 5;\"\n",
      "my_stdb.semanticSelect('laws', statement, 'view_summaries', 1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "['2813, provided that: \\xe2\\x80\\x9cExcept as expressly provided otherwise, any reference to \\xe2\\x80\\x98this Act\\xe2\\x80\\x99 contained in either division A [Department of Defense Appropriations Act, 2006, see Tables for classification] or division B [Emergency Supplemental Appropriations Act to Address Hurricanes in the Gulf of Mexico and Pandemic Influenza, 2006, see Tables for classification] shall be treated as referring only to the provisions of that division.\\xe2\\x80\\x9d',\n",
        " 'The resolving clause of all joint resolutions shall be in the following form: \\xe2\\x80\\x9cResolved by the Senate and House of Representatives of the United States of America in Congress assembled.\\xe2\\x80\\x9d (July 30, 1947, ch.',\n",
        " 'No enacting or resolving words shall be used in any section of an Act or resolution of Congress except in the first.',\n",
        " 'Each section shall be numbered, and shall contain, as nearly as may be, a single proposition of enactment.']"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import NLPfunctions as nlpf\n",
      "nlpf.correct_spelling(\"au ll main ayunan tante,  to london ah? of @zachzachra: Seat sekolahannya si @Indrarbk\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "TextBlob(\"au ll main ayunan tante,  to london ah? of @zachzachra: Seat sekolahannya si @Indrarbk\")"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "statement = \"select content from twitter_text where id < 10;\"\n",
      "my_stdb.semanticSelect('twitter_text', statement, 'correct_spelling')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "['au ll main ayunan tante,  to london ah? of @zachzachra: Seat sekolahannya si @Indrarbk',\n",
        " 'A pink taxi covered in flags? Seriously? To glad him getting out of London tonight...',\n",
        " '@ShelaghFogarty you will be missed in the mornings. Took forward to your new show!',\n",
        " 'http://www.book-limb.com/money-machine-penguin-business-library.html #london #stock #exchange #let Money Machine (Penguin Business Library',\n",
        " 'Midday Supervisor: Midday Supervisor at London Borough of Waltham Forest in Waltham Forest,  Greater London. Cal... http://bit.ly/ifotPo',\n",
        " 'Moves waking up in London:-)',\n",
        " 'of @RoyalsWed: Must 24 hours until @news & @online go LIVE from London for the #RoyalWedding! http://only.ne/ROYALS #rw2011',\n",
        " 'him at Starbucks (London Waterloo Station (WAT),  London) w/ 2 others http://esq.com/lpZkfo',\n",
        " 'since so and royal wedding again, curious a king piano b and busy no ma royalties a look no buckingham palace sa London? @kuyakim_atienza']"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t = time.time()\n",
      "cur.execute(\"select * from laws\")\n",
      "#result = cur.fetchall()\n",
      "print \"entire thing:\", time.time() - t"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(10001L,)]\n",
        "entire thing: 0.029000043869\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t = time.time()\n",
      "cur.execute(\"select * from laws limit 2\")\n",
      "#result = cur.fetchall()\n",
      "print \"limit 2 things only:\", time.time() - t"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "limit 2 things only: 0.00999999046326\n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#If you ever end a transaction while its running, the following two commands be necessary to reset for next transaction.\n",
      "cur.execute(\"END;\")\n",
      "cur.execute(\"ABORT;\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Using our machine-generated tables. Everytime you insert a document, machine generated tables are created. One of these tables is called \"tablename_text.\" This table contains the actual text for each document you've inserted. For example the twitter table and twitter_text table can be joined by id. Thus, you can efficiently find documents of interest in the twitter table, then view the text in the twitter_text table. This allows for efficient manipulation of tables containing many documents without handling all of the text for each document simultaneously. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cur.execute(\"select * from twitter_text limit 5;\")\n",
      "cur.fetchall()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 58,
       "text": [
        "[(1,\n",
        "  'aku lg main ayunan tante,  tk london yah? RT @zachzachra: Lewat sekolahannya si @Indrarbk'),\n",
        " (2,\n",
        "  \"A pink taxi covered in flags? Seriously? So glad I'm getting out of London tonight...\"),\n",
        " (3,\n",
        "  '@ShelaghFogarty you will be missed in the mornings. Look forward to your new show!'),\n",
        " (4,\n",
        "  'http://www.book-lib.com/money-machine-penguin-business-library.html #london #stock #exchange #lex Money Machine (Penguin Business Librar'),\n",
        " (5,\n",
        "  'Midday Supervisor: Midday Supervisor at London Borough of Waltham Forest in Waltham Forest,  Greater London. Scal... http://bit.ly/ifotPo')]"
       ]
      }
     ],
     "prompt_number": 58
    }
   ],
   "metadata": {}
  }
 ]
}