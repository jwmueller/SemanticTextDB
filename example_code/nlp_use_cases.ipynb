{
 "metadata": {
  "name": "",
  "signature": "sha256:89bb32d366571e93bcf0e7a8b11b70f95254a89d7233691bc74f63f35fe14854"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Determining sentiment of Kate Middleton in London as she marries into royalty."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import csv\n",
      "from textblob import TextBlob"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def mycsv_reader(csv_reader): \n",
      "  while True: \n",
      "    try: \n",
      "      yield next(csv_reader) \n",
      "    except csv.Error: \n",
      "      print \"missed value\"\n",
      "      pass\n",
      "    continue "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "redditReadPath = \"C:/git/SemanticTextDB/example_code/twitter.csv\"\n",
      "count = 0\n",
      "posCount = 0\n",
      "negCount = 0\n",
      "with open(redditReadPath, 'rU') as csvfile:\n",
      "    reader = mycsv_reader(csv.reader(csvfile))\n",
      "    for row in reader:\n",
      "        count = count + 1\n",
      "        try:           \n",
      "            twitterID = row.pop(0)\n",
      "            location = row.pop(0)\n",
      "            username = row.pop(-1)\n",
      "            tweet = ', '.join(row)\n",
      "        except:\n",
      "            #print \"Error occurred at item\", count, \"skipping insertion.\"\n",
      "            continue\n",
      "\n",
      "        #Perform textblob stuff here\n",
      "        #--------------------------------------------------        \n",
      "        NLPObject = TextBlob(unicode(tweet, 'utf-8'))\n",
      "        if NLPObject.sentiment.polarity <= -.8 and 'wedding' in tweet:\n",
      "            negCount = negCount + 1\n",
      "            #print tweet\n",
      "            #print \"Sentiment Score:\", NLPObject.sentiment.polarity\n",
      "            #print\n",
      "        if NLPObject.sentiment.polarity >= .8 and 'wedding' in tweet:\n",
      "            posCount = posCount + 1\n",
      "            #print tweet\n",
      "            #print \"Sentiment Score:\", NLPObject.sentiment.polarity\n",
      "            #print\n",
      "        #--------------------------------------------------\n",
      "        #print count\n",
      "        #print \"London support of Kate Middleton is at least:\", 1.0*posCount/negCount, \"supporters for every 1 non-supporter.\" \n",
      "print count\n",
      "print \"London support of Kate Middleton is at least:\", 1.0*posCount/negCount, \"supporters for every 1 non-supporter.\" "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Reading in documents and printing their summary"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# -*- coding: utf8 -*-\n",
      "import os\n",
      "\n",
      "from __future__ import absolute_import\n",
      "from __future__ import division, print_function\n",
      "\n",
      "from sumy.parsers.plaintext import PlaintextParser\n",
      "from sumy.nlp.tokenizers import Tokenizer\n",
      "from sumy.summarizers.lsa import LsaSummarizer as Summarizer\n",
      "from sumy.nlp.stemmers import Stemmer\n",
      "from sumy.utils import get_stop_words\n",
      "\n",
      "#Add the following two lines if you have the following error (NLTK not installed):\n",
      "#Resource 'tokenizers/punkt/english.pickle' not found.  Please\n",
      "#use the NLTK Downloader to obtain the resource:\n",
      "#import nltk\n",
      "#nltk.download() #Opens a windows GUI installer for NLTK\n",
      "\n",
      "\n",
      "LANGUAGE = \"english\"\n",
      "SENTENCES_COUNT = 1\n",
      "\n",
      "readPath = \"C:/git/SemanticTextDB/example_code/all_US_Law_Codes/\"\n",
      "count = 0\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    for filename in os.listdir(readPath):\n",
      "        count = count + 1\n",
      "        parser = PlaintextParser.from_file(readPath + filename, Tokenizer(LANGUAGE))\n",
      "        stemmer = Stemmer(LANGUAGE)\n",
      "\n",
      "        summarizer = Summarizer(stemmer)\n",
      "        summarizer.stop_words = get_stop_words(LANGUAGE)\n",
      "           \n",
      "        summary = \". \".join([str(sentence) for sentence in summarizer(parser.document, SENTENCES_COUNT)])\n",
      "        print(summary)\n",
      "        print(\"\\n\")\n",
      "            \n",
      "        if count > 3:\n",
      "            break"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}